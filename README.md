# shiny-rotary-phone
Attempt to improve on the previous neural network implementation by adding multiple layers and use backpropagation
  The code is attempting to recreate the dense sequential model provided by tensorflow and keras. It defines a dense_net class that is a fully connected multilayer perceptron model. The class takes in a tuple containing the dimensions of the input, hidden, and output layers. The model implements backpropagation according to my understanding of it that I acquired from wikipedia, my previous exposure, and CS12. In its current state it seems like there is a lot of optimization to do, since it is having trouble learning to classify an extremely simple 2-D dataset. It is also very sensitive to the learning rate, since it seems like it either always blows up or converges extremely slowly to a solution. The google colllab has an example of the network converging extremely slowly. There might be something wrong with the code.
  I also implemented a convolution function because I thought that I would get the backpropagation working perfectly on the fully connected multilayer perceptron. It was fun to play around with it though.
